[{"start_min": "0:07", "start_sec": 7.12, "value": "giving an intuitive walkthrough of the backpropagation algorithm."}, {"start_min": "0:21", "start_sec": 21.92, "value": "Our main goal is to show how people in machine learningcommonly think about the chain rule from the calculus in the context of networks,"}, {"start_min": "0:25", "start_sec": 25.18, "value": ""}, {"start_min": "0:29", "start_sec": 29.44, "value": ""}, {"start_min": "0:40", "start_sec": 40.34, "value": "Let\u2019s just start off with an extremely simple network,one where each layer has a single neuron in it."}, {"start_min": "0:50", "start_sec": 50.68, "value": "and our goal is to understand how sensitive the cost function is to these variables."}, {"start_min": "1:05", "start_sec": 65.88, "value": "Let's label the activation of that last neuron a with a superscript L, indicating which layer it\u2019s in,"}, {"start_min": "1:32", "start_sec": 92.94, "value": "So the cost of this simple network for a single training example is (a^(L) - y)^2."}, {"start_min": "2:01", "start_sec": 121.85, "value": "It's actually going to make things easier for us if we give a special name to this weighted sum, like z,with the same superscript as the relevant activations."}, {"start_min": "2:11", "start_sec": 131.48, "value": "And a way you might conceptualize this is that the weight, the previous activation, and the biasaltogether are used to compute z, which in turn lets us compute a,"}, {"start_min": "2:43", "start_sec": 163.99, "value": "how sensitive the cost function is to small changes in our weight w^(L)."}, {"start_min": "3:16", "start_sec": 196.52, "value": "which in turn causes some change to a^(L), which directly influences the cost."}, {"start_min": "3:47", "start_sec": 227.85, "value": "where multiplying together these three ratios gives us the sensitivity of C to small changes in w^(L)."}, {"start_min": "4:03", "start_sec": 243.6, "value": "because now we are gonna compute the relevant derivatives."}, {"start_min": "4:23", "start_sec": 263.34, "value": "even slight changes stand to have a big impact on the cost function."}, {"start_min": "4:49", "start_sec": 289.57, "value": "without taking a moment to sit back and remind yourself what they all actually mean."}, {"start_min": "4:56", "start_sec": 296.04, "value": "the amount that a small nudge to this weight influences the last layerdepends on how strong the previous neuron is."}, {"start_min": "5:09", "start_sec": 309.21, "value": "And all of this is the derivative with respect to w^(L) only of the cost for a specific training example."}, {"start_min": "5:22", "start_sec": 322.15, "value": "its derivative requires averaging this expression that we found over all training examples."}, {"start_min": "5:33", "start_sec": 333.89, "value": "the partial derivatives of the cost function with respect to all those weights and biases."}, {"start_min": "6:10", "start_sec": 370.23, "value": "you can see how sensitive this cost function is to the activation of the previous layer;"}, {"start_min": "6:33", "start_sec": 393.08, "value": "because now we can just keep iterating this chain rule idea backwardsto see how sensitive the cost function is to previous weights and to previous biases."}, {"start_min": "6:47", "start_sec": 407.88, "value": "and things are just gonna get exponentially more complicated in the real network."}, {"start_min": "6:51", "start_sec": 411.68, "value": "Really it's just a few more indices to keep track of."}, {"start_min": "7:19", "start_sec": 439.38, "value": "we add up the squares of the differences between these last layer activations and the desired output."}, {"start_min": "7:48", "start_sec": 468.26, "value": "but it lines up with how you\u2019d index the weight matrix that I talked about in the Part 1 video."}, {"start_min": "7:58", "start_sec": 478.35, "value": "so that the activation of the last layer is just your special function, like the sigmoid, applied to z."}, {"start_min": "8:37", "start_sec": 517.76, "value": "In this case, the difference is the neuron influences the cost function through multiple paths."}, {"start_min": "8:44", "start_sec": 524.66, "value": ""}, {"start_min": "9:08", "start_sec": 548.84, "value": "you can just repeat the process for all the weights and biases feeding into that layer."}, {"start_min": "9:23", "start_sec": 563.59, "value": "These chain rule expressions give you the derivatives that determine each component in the gradientthat helps minimize the cost of the network by repeatedly stepping downhill."}, {"start_min": "9:29", "start_sec": 569.3, "value": ""}]