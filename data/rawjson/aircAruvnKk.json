[{"start":"4.02","dur":"6.66","text":"This is a three. It's sloppily written and rendered at an extremely low resolution of 28 by 28 pixels."},{"start":"10.68","dur":"4.98","text":"But your brain has no trouble recognizing it as a three and I want you to take a moment to appreciate"},{"start":"15.9","dur":"3.049","text":"How crazy it is that brains can do this so effortlessly?"},{"start":"18.949","dur":"4.211","text":"I mean this this and this are also recognizable as threes,"},{"start":"23.16","dur":"4.9","text":"even though the specific values of each pixel is very different from one image to the next."},{"start":"28.08","dur":"5.7","text":"The particular light-sensitive cells in your eye that are firing when you see this three"},{"start":"33.78","dur":"3.02","text":"are very different from the ones firing when you see this three."},{"start":"37.14","dur":"3.47","text":"But something in that crazy smart visual cortex of yours"},{"start":"41.129","dur":"7.01","text":"resolves these as representing the same idea while at the same time recognizing other images as their own distinct ideas"},{"start":"48.84","dur":"6.199","text":"But if I told you hey sit down and write for me a program that takes in a grid of 28 by 28"},{"start":"55.379","dur":"6.38","text":"pixels like this and outputs a single number between 0 and 10 telling you what it thinks the digit is"},{"start":"62.25","dur":"3.889","text":"Well the task goes from comically trivial to dauntingly difficult"},{"start":"66.75","dur":"1.52","text":"Unless you've been living under a rock"},{"start":"68.27","dur":"6.329","text":"I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present into the future"},{"start":"74.64","dur":"3.77","text":"But what I want to do here is show you what a neural network actually is"},{"start":"78.66","dur":"5.569","text":"Assuming no background and to help visualize what it's doing not as a buzzword but as a piece of math"},{"start":"84.57","dur":"3.74","text":"My hope is just that you come away feeling like this structure itself is"},{"start":"88.38","dur":"6.019","text":"Motivated and to feel like you know what it means when you read or you hear about a neural network quote-unquote learning"},{"start":"94.95","dur":"5.299","text":"This video is just going to be devoted to the structure component of that and the following one is going to tackle learning"},{"start":"100.53","dur":"5.42","text":"What we're going to do is put together a neural network that can learn to recognize handwritten digits"},{"start":"109.27","dur":"2.059","text":"This is a somewhat classic example for"},{"start":"111.52","dur":"5.239","text":"Introducing the topic and I'm happy to stick with the status quo here because at the end of the two videos I want to point"},{"start":"116.76","dur":"5.339","text":"You to a couple good resources where you can learn more and where you can download the code that does this and play with it?"},{"start":"122.1","dur":"2","text":"on your own computer"},{"start":"124.75","dur":"4.22","text":"There are many many variants of neural networks and in recent years"},{"start":"128.97","dur":"3","text":"There's been sort of a boom in research towards these variants"},{"start":"132.13","dur":"6.889","text":"But in these two introductory videos you and I are just going to look at the simplest plain-vanilla form with no added frills"},{"start":"139.3","dur":"1.74","text":"This is kind of a necessary"},{"start":"141.04","dur":"3.47","text":"prerequisite for understanding any of the more powerful modern variants and"},{"start":"144.76","dur":"3.439","text":"Trust me it still has plenty of complexity for us to wrap our minds around"},{"start":"148.69","dur":"4.13","text":"But even in this simplest form it can learn to recognize handwritten digits"},{"start":"152.82","dur":"3.36","text":"Which is a pretty cool thing for a computer to be able to do."},{"start":"157.12","dur":"4.84","text":"And at the same time you'll see how it does fall short of a couple hopes that we might have for it"},{"start":"163.09","dur":"5.089","text":"As the name suggests neural networks are inspired by the brain, but let's break that down"},{"start":"168.52","dur":"2.869","text":"What are the neurons and in what sense are they linked together?"},{"start":"172.09","dur":"5.66","text":"Right now when I say neuron all I want you to think about is a thing that holds a number"},{"start":"178.209","dur":"3.92","text":"Specifically a number between 0 & 1 it's really not more than that"},{"start":"183.43","dur":"7.7","text":"For example the network starts with a bunch of neurons corresponding to each of the 28 times 28 pixels of the input image"},{"start":"191.4","dur":"1.06","text":"which is"},{"start":"192.46","dur":"7.78","text":"784 neurons in total each one of these holds a number that represents the grayscale value of the corresponding pixel"},{"start":"200.769","dur":"3.53","text":"ranging from 0 for black pixels up to 1 for white pixels"},{"start":"204.91","dur":"5.509","text":"This number inside the neuron is called its activation and the image you might have in mind here"},{"start":"210.42","dur":"3.539","text":"Is that each neuron is lit up when its activation is a high number?"},{"start":"216.26","dur":"5.299","text":"So all of these 784 neurons make up the first layer of our network"},{"start":"225.99","dur":"5.299","text":"Now jumping over to the last layer this has ten neurons each representing one of the digits"},{"start":"231.57","dur":"4.669","text":"the activation in these neurons again some number that's between zero and one"},{"start":"236.88","dur":"3.169","text":"Represents how much the system thinks that a given image?"},{"start":"240.72","dur":"5.27","text":"Corresponds with a given digit. There's also a couple layers in between called the hidden layers"},{"start":"246.18","dur":"1.59","text":"Which for the time being?"},{"start":"247.77","dur":"5.779","text":"Should just be a giant question mark for how on earth this process of recognizing digits is going to be handled"},{"start":"253.74","dur":"6.469","text":"In this network I chose two hidden layers each one with 16 neurons and admittedly that's kind of an arbitrary choice"},{"start":"260.609","dur":"4.28","text":"to be honest I chose two layers based on how I want to motivate the structure in just a moment and"},{"start":"265.35","dur":"3.829","text":"16 well that was just a nice number to fit on the screen in practice"},{"start":"269.18","dur":"3.029","text":"There is a lot of room for experiment with a specific structure here"},{"start":"272.73","dur":"5.599","text":"The way the network operates activations in one layer determine the activations of the next layer"},{"start":"278.76","dur":"6.589","text":"And of course the heart of the network as an information processing mechanism comes down to exactly how those"},{"start":"285.57","dur":"2.839","text":"activations from one layer bring about activations in the next layer"},{"start":"288.9","dur":"5.959","text":"It's meant to be loosely analogous to how in biological networks of neurons some groups of neurons firing"},{"start":"295.41","dur":"2","text":"cause certain others to fire"},{"start":"297.57","dur":"0.77","text":"Now the network"},{"start":"298.34","dur":"4.679","text":"I'm showing here has already been trained to recognize digits and let me show you what I mean by that"},{"start":"303.14","dur":"3.44","text":"It means if you feed in an image lighting up all"},{"start":"306.64","dur":"5.14","text":"784 neurons of the input layer according to the brightness of each pixel in the image"},{"start":"312.33","dur":"4.699","text":"That pattern of activations causes some very specific pattern in the next layer"},{"start":"317.19","dur":"2.119","text":"Which causes some pattern in the one after it?"},{"start":"319.44","dur":"2.75","text":"Which finally gives some pattern in the output layer and?"},{"start":"322.35","dur":"7.009","text":"The brightest neuron of that output layer is the network's choice so to speak for what digit this image represents?"},{"start":"332.07","dur":"4.789","text":"And before jumping into the math for how one layer influences the next or how training works?"},{"start":"337.14","dur":"5.929","text":"Let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently"},{"start":"343.8","dur":"4.46","text":"What are we expecting here? What is the best hope for what those middle layers might be doing?"},{"start":"348.86","dur":"7.86","text":"Well when you or I recognize digits we piece together various components a nine has a loop up top and a line on the right"},{"start":"357.26","dur":"4.02","text":"an 8 also has a loop up top, but it's paired with another loop down low"},{"start":"362.02","dur":"4.579","text":"A 4 basically breaks down into three specific lines and things like that"},{"start":"367.18","dur":"4.79","text":"Now in a perfect world we might hope that each neuron in the second-to-last layer"},{"start":"372.64","dur":"2.089","text":"corresponds with one of these sub components"},{"start":"374.89","dur":"4.85","text":"That anytime you feed in an image with say a loop up top like a 9 or an 8"},{"start":"379.87","dur":"1.35","text":"There's some specific"},{"start":"381.22","dur":"6.529","text":"Neuron whose activation is going to be close to one and I don't mean this specific loop of pixels the hope would be that any"},{"start":"388.09","dur":"6.949","text":"Generally loopy pattern towards the top sets off this neuron that way going from the third layer to the last one"},{"start":"395.38","dur":"4.58","text":"just requires learning which combination of sub components corresponds to which digits"},{"start":"400.51","dur":"2.3","text":"Of course that just kicks the problem down the road"},{"start":"402.91","dur":"6.109","text":"Because how would you recognize these sub components or even learn what the right sub components should be and I still haven't even talked about"},{"start":"409.02","dur":"3.809","text":"How one layer influences the next but run with me on this one for a moment"},{"start":"413.65","dur":"2.69","text":"recognizing a loop can also break down into subproblems"},{"start":"416.86","dur":"5.69","text":"One reasonable way to do this would be to first recognize the various little edges that make it up"},{"start":"423.52","dur":"5.39","text":"Similarly a long line like the kind you might see in the digits 1 or 4 or 7"},{"start":"428.91","dur":"5.369","text":"Well that's really just a long edge or maybe you think of it as a certain pattern of several smaller edges"},{"start":"434.74","dur":"4.639","text":"So maybe our hope is that each neuron in the second layer of the network"},{"start":"440.29","dur":"2.36","text":"corresponds with the various relevant little edges"},{"start":"443.23","dur":"5.029","text":"Maybe when an image like this one comes in it lights up all of the neurons"},{"start":"448.72","dur":"2.929","text":"associated with around eight to ten specific little edges"},{"start":"451.93","dur":"5","text":"which in turn lights up the neurons associated with the upper loop and a long vertical line and"},{"start":"457.3","dur":"2.299","text":"Those light up the neuron associated with a nine"},{"start":"460.3","dur":"0.8","text":"whether or not"},{"start":"461.1","dur":"5.97","text":"This is what our final network actually does is another question, one that I'll come back to once we see how to train the network"},{"start":"467.35","dur":"4.82","text":"But this is a hope that we might have. A sort of goal with the layered structure like this"},{"start":"473.02","dur":"6.32","text":"Moreover you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks"},{"start":"479.74","dur":"7.009","text":"And even beyond image recognition there are all sorts of intelligent things you might want to do that break down into layers of abstraction"},{"start":"487.69","dur":"6.98","text":"Parsing speech for example involves taking raw audio and picking out distinct sounds which combine to make certain syllables"},{"start":"495.07","dur":"4.759","text":"Which combine to form words which combine to make up phrases and more abstract thoughts etc"},{"start":"500.77","dur":"4.94","text":"But getting back to how any of this actually works picture yourself right now designing"},{"start":"505.71","dur":"4.739","text":"How exactly the activations in one layer might determine the activations in the next?"},{"start":"510.67","dur":"5.209","text":"The goal is to have some mechanism that could conceivably combine pixels into edges"},{"start":"515.88","dur":"5.55","text":"Or edges into patterns or patterns into digits and to zoom in on one very specific example"},{"start":"521.95","dur":"2.239","text":"Let's say the hope is for one particular"},{"start":"524.38","dur":"6.05","text":"Neuron in the second layer to pick up on whether or not the image has an edge in this region here"},{"start":"530.95","dur":"4.01","text":"The question at hand is what parameters should the network have"},{"start":"535.27","dur":"7.22","text":"what dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern or"},{"start":"542.59","dur":"4.7","text":"Any other pixel pattern or the pattern that several edges can make a loop and other such things?"},{"start":"548.29","dur":"7.099","text":"Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer"},{"start":"555.85","dur":"2","text":"These weights are just numbers"},{"start":"558.19","dur":"7.4","text":"then take all those activations from the first layer and compute their weighted sum according to these weights I"},{"start":"567.37","dur":"4.31","text":"Find it helpful to think of these weights as being organized into a little grid of their own"},{"start":"571.68","dur":"5.399","text":"And I'm going to use green pixels to indicate positive weights and red pixels to indicate negative weights"},{"start":"577.24","dur":"4.43","text":"Where the brightness of that pixel is some loose depiction of the weights value?"},{"start":"582.4","dur":"3.44","text":"Now if we made the weights associated with almost all of the pixels zero"},{"start":"586.15","dur":"2.929","text":"except for some positive weights in this region that we care about"},{"start":"589.48","dur":"1.83","text":"then taking the weighted sum of"},{"start":"591.31","dur":"6.38","text":"all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about"},{"start":"598.87","dur":"5.57","text":"And, if you really want it to pick up on whether there's an edge here what you might do is have some negative weights"},{"start":"604.9","dur":"2","text":"associated with the surrounding pixels"},{"start":"607.03","dur":"5.63","text":"Then the sum is largest when those middle pixels are bright, but the surrounding pixels are darker"},{"start":"614.279","dur":"3.89","text":"When you compute a weighted sum like this you might come out with any number"},{"start":"618.24","dur":"4.94","text":"but for this network what we want is for activations to be some value between 0 & 1"},{"start":"623.73","dur":"2.869","text":"so a common thing to do is to pump this weighted sum"},{"start":"626.91","dur":"5.09","text":"Into some function that squishes the real number line into the range between 0 & 1 and"},{"start":"632.19","dur":"5.059","text":"A common function that does this is called the sigmoid function also known as a logistic curve"},{"start":"637.98","dur":"5.359","text":"basically very negative inputs end up close to zero very positive inputs end up close to 1"},{"start":"643.339","dur":"3.059","text":"and it just steadily increases around the input 0"},{"start":"649.08","dur":"6.949","text":"So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is"},{"start":"657.45","dur":"4.369","text":"But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0"},{"start":"662.1","dur":"4.16","text":"Maybe you only want it to be active when the sum is bigger than say 10"},{"start":"666.63","dur":"3.649","text":"That is you want some bias for it to be inactive"},{"start":"670.86","dur":"5.239","text":"what we'll do then is just add in some other number like negative 10 to this weighted sum"},{"start":"676.529","dur":"3.14","text":"Before plugging it through the sigmoid squishification function"},{"start":"680.22","dur":"2.51","text":"That additional number is called the bias"},{"start":"683.31","dur":"5.75","text":"So the weights tell you what pixel pattern this neuron in the second layer is picking up on and the bias"},{"start":"689.22","dur":"6.23","text":"tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active"},{"start":"695.91","dur":"2","text":"And that is just one neuron"},{"start":"698.12","dur":"3.82","text":"Every other neuron in this layer is going to be connected to all"},{"start":"702.32","dur":"8.3","text":"784 pixels neurons from the first layer and each one of those 784 connections has its own weight associated with it"},{"start":"711.33","dur":"6.409","text":"also each one has some bias some other number that you add on to the weighted sum before squishing it with the sigmoid and"},{"start":"718.02","dur":"3.889","text":"That's a lot to think about with this hidden layer of 16 neurons"},{"start":"722.01","dur":"6.26","text":"that's a total of 784 times 16 weights along with 16 biases"},{"start":"728.49","dur":"5.539","text":"And all of that is just the connections from the first layer to the second the connections between the other layers"},{"start":"734.029","dur":"3.179","text":"Also, have a bunch of weights and biases associated with them"},{"start":"737.76","dur":"2.92","text":"All said and done this network has almost exactly"},{"start":"741.28","dur":"2.64","text":"13,000 total weights and biases"},{"start":"744.28","dur":"5.26","text":"13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways"},{"start":"750.52","dur":"2","text":"So when we talk about learning?"},{"start":"752.53","dur":"7.669","text":"What that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve"},{"start":"760.2","dur":"1.99","text":"the problem at hand"},{"start":"762.19","dur":"0.81","text":"one thought"},{"start":"763","dur":"6.979","text":"Experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand"},{"start":"770.38","dur":"5.779","text":"Purposefully tweaking the numbers so that the second layer picks up on edges the third layer picks up on patterns etc"},{"start":"776.35","dur":"5.09","text":"I personally find this satisfying rather than just reading the network as a total black box"},{"start":"781.87","dur":"2.479","text":"Because when the network doesn't perform the way you"},{"start":"784.6","dur":"6.77","text":"anticipate if you've built up a little bit of a relationship with what those weights and biases actually mean you have a starting place for"},{"start":"791.68","dur":"4.609","text":"Experimenting with how to change the structure to improve or when the network does work?"},{"start":"796.29","dur":"2","text":"But not for the reasons you might expect"},{"start":"798.31","dur":"6.859","text":"Digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible"},{"start":"805.18","dur":"1.17","text":"solutions"},{"start":"806.35","dur":"4.25","text":"By the way the actual function here is a little cumbersome to write down. Don't you think?"},{"start":"812.35","dur":"6.11","text":"So let me show you a more notationally compact way that these connections are represented. This is how you'd see it"},{"start":"818.46","dur":"2","text":"If you choose to read up more about neural networks"},{"start":"821.11","dur":"4.7","text":"Organize all of the activations from one layer into a column as a vector"},{"start":"827.47","dur":"4.85","text":"Then organize all of the weights as a matrix where each row of that matrix"},{"start":"832.9","dur":"4.759","text":"corresponds to the connections between one layer and a particular neuron in the next layer"},{"start":"838.06","dur":"5.539","text":"What that means is that taking the weighted sum of the activations in the first layer according to these weights?"},{"start":"844","dur":"5.33","text":"Corresponds to one of the terms in the matrix vector product of everything we have on the left here"},{"start":"853.54","dur":"4.84","text":"By the way so much of machine learning just comes down to having a good grasp of linear algebra"},{"start":"858.38","dur":"8.56","text":"So for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means take a look at the series I did on linear algebra"},{"start":"867.25","dur":"1.589","text":"especially chapter three"},{"start":"868.839","dur":"6.92","text":"Back to our expression instead of talking about adding the bias to each one of these values independently we represent it by"},{"start":"876.01","dur":"6.199","text":"Organizing all those biases into a vector and adding the entire vector to the previous matrix vector product"},{"start":"882.91","dur":"1.13","text":"Then as a final step"},{"start":"884.04","dur":"3.21","text":"I'll rap a sigmoid around the outside here"},{"start":"887.25","dur":"4.649","text":"And what that's supposed to represent is that you're going to apply the sigmoid function to each specific"},{"start":"892.42","dur":"2.15","text":"component of the resulting vector inside"},{"start":"895.51","dur":"5.239","text":"So once you write down this weight matrix and these vectors as their own symbols you can"},{"start":"901","dur":"6.589","text":"communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression and"},{"start":"907.93","dur":"7.07","text":"This makes the relevant code both a lot simpler and a lot faster since many libraries optimize the heck out of matrix multiplication"},{"start":"917.56","dur":"3.799","text":"Remember how earlier I said these neurons are simply things that hold numbers"},{"start":"921.79","dur":"4.46","text":"Well of course the specific numbers that they hold depends on the image you feed in"},{"start":"927.79","dur":"5.15","text":"So it's actually more accurate to think of each neuron as a function one that takes in the"},{"start":"933.07","dur":"5","text":"outputs of all the neurons in the previous layer and spits out a number between zero and one"},{"start":"938.8","dur":"3.47","text":"Really the entire network is just a function one that takes in"},{"start":"942.76","dur":"4.25","text":"784 numbers as an input and spits out ten numbers as an output"},{"start":"947.47","dur":"1.23","text":"It's an absurdly"},{"start":"948.7","dur":"7.549","text":"Complicated function one that involves thirteen thousand parameters in the forms of these weights and biases that pick up on certain patterns and which involves"},{"start":"956.25","dur":"4.02","text":"iterating many matrix vector products and the sigmoid squish evocation function"},{"start":"960.61","dur":"5.78","text":"But it's just a function nonetheless and in a way it's kind of reassuring that it looks complicated"},{"start":"966.39","dur":"5.849","text":"I mean if it were any simpler what hope would we have that it could take on the challenge of recognizing digits?"},{"start":"972.96","dur":"6.599","text":"And how does it take on that challenge? How does this network learn the appropriate weights and biases just by looking at data? Oh?"},{"start":"980.08","dur":"5.959","text":"That's what I'll show in the next video, and I'll also dig a little more into what this particular network we are seeing is really doing"},{"start":"987.13","dur":"5.51","text":"Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out"},{"start":"992.76","dur":"4.8","text":"But realistically most of you don't actually receive notifications from YouTube, do you ?"},{"start":"997.56","dur":"4.7","text":"Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's"},{"start":"1002.459","dur":"5.18","text":"Recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you"},{"start":"1008.25","dur":"2","text":"anyway stay posted for more"},{"start":"1010.41","dur":"3.14","text":"Thank you very much to everyone supporting these videos on patreon"},{"start":"1013.589","dur":"3.17","text":"I've been a little slow to progress in the probability series this summer"},{"start":"1016.76","dur":"4.619","text":"But I'm jumping back into it after this project so patrons you can look out for updates there"},{"start":"1023.31","dur":"2.24","text":"To close things off here I have with me Lisha Li"},{"start":"1025.55","dur":"6.479","text":"Lee who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called amplify partners"},{"start":"1032.03","dur":"4.5","text":"Who kindly provided some of the funding for this video so Lisha one thing"},{"start":"1036.53","dur":"2.579","text":"I think we should quickly bring up is this sigmoid function"},{"start":"1039.18","dur":"5.6","text":"As I understand it early networks used this to squish the relevant weighted sum into that interval between zero and one"},{"start":"1044.98","dur":"5.36","text":"You know kind of motivated by this biological analogy of neurons either being inactive or active\n(Lisha) - Exactly"},{"start":"1050.36","dur":"5.96","text":"(3B1B) - But relatively few modern networks actually use sigmoid anymore. That's kind of old school right ?\n(Lisha) - Yeah or rather"},{"start":"1056.37","dur":"6.41","text":"ReLU seems to be much easier to train\n(3B1B) - And ReLU really stands for rectified linear unit"},{"start":"1062.78","dur":"6.059","text":"(Lisha) - Yes it's this kind of function where you're just taking a max of 0 and a where a is given by"},{"start":"1069.12","dur":"4.55","text":"what you were explaining in the video and what this was sort of motivated from I think was a"},{"start":"1074.61","dur":"2","text":"partially by a biological"},{"start":"1076.62","dur":"1.559","text":"Analogy with how"},{"start":"1078.179","dur":"4.91","text":"Neurons would either be activated or not and so if it passes a certain threshold"},{"start":"1083.25","dur":"2","text":"It would be the identity function"},{"start":"1085.29","dur":"5.149","text":"But if it did not then it would just not be activated so be zero so it's kind of a simplification"},{"start":"1090.72","dur":"3.709","text":"Using sigmoids didn't help training, or it was very difficult to train"},{"start":"1094.429","dur":"5.16","text":"It's at some point and people just tried relu and it happened to work"},{"start":"1100.11","dur":"2.03","text":"Very well for these incredibly"},{"start":"1102.69","dur":"2.4","text":"Deep neural networks.\n(3B1B) - All right"},{"start":"1105.09","dur":"0.97","text":"Thank You Lisha"},{"start":"1106.06","dur":"7.369","text":"for background amplify partners in early-stage VC invests in technical founders building the next generation of companies focused on the"},{"start":"1113.59","dur":"4.819","text":"applications of AI if you or someone that you know has ever thought about starting a company someday"},{"start":"1118.45","dur":"4.729","text":"Or if you're working on an early-stage one right now the Amplify folks would love to hear from you"},{"start":"1123.24","dur":"5.56","text":"they even set up a specific email for this video 3blue1brown@amplifypartners.com"},{"start":"1128.8","dur":"1.98","text":"so feel free to reach out to them through that"},{"start":"1152.13","dur":"2.06","text":" "}]