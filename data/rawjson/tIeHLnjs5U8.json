[{"start":"4.23","dur":"2.89","text":"The hard assumption here is that you’ve watched part 3,"},{"start":"7.12","dur":"3.11","text":"giving an intuitive walkthrough of the backpropagation algorithm."},{"start":"11.04","dur":"3.73","text":"Here, we get a bit more formal and dive into the relevant calculus."},{"start":"14.77","dur":"2.27","text":"It’s normal for this to be a little confusing,"},{"start":"17.04","dur":"4.44","text":"so the mantra to regularly pause and ponder certainly applies as much here as anywhere else."},{"start":"21.92","dur":"3.26","text":"Our main goal is to show how people in machine learning"},{"start":"25.18","dur":"4.26","text":"commonly think about the chain rule from the calculus in the context of networks,"},{"start":"29.44","dur":"4.38","text":"which has a different feel for how much most introductory calculus courses approach the subject."},{"start":"34.5","dur":"2.39","text":"For those of you uncomfortable with the relevant calculus,"},{"start":"36.89","dur":"2.15","text":"I do have a whole series on the topic."},{"start":"40.34","dur":"2.81","text":"Let’s just start off with an extremely simple network,"},{"start":"43.15","dur":"2.58","text":"one where each layer has a single neuron in it."},{"start":"46.27","dur":"4.41","text":"So this particular network is determined by 3 weights and 3 biases,"},{"start":"50.68","dur":"4.39","text":"and our goal is to understand how sensitive the cost function is to these variables."},{"start":"55.55","dur":"2.28","text":"That way we know which adjustments to these terms"},{"start":"57.83","dur":"3.11","text":"is going to cause the most efficient decrease to the cost function."},{"start":"61.92","dur":"3.25","text":"And we're just focus on the connection between the last two neurons."},{"start":"65.88","dur":"5.49","text":"Let's label the activation of that last neuron a with a superscript L, indicating which layer it’s in,"},{"start":"71.69","dur":"4.03","text":"so the activation of this previous neuron is a^(L-1)."},{"start":"76.43","dur":"3.6","text":"There are not exponents, they're just a way of indexing what we’re talking about,"},{"start":"80.03","dur":"2.94","text":"since I want to save subscripts for different indices later on."},{"start":"83.74","dur":"5.97","text":"Let’s say that the value we want this last activation to be for a given training example is y."},{"start":"90.17","dur":"2.19","text":"For example, y might be 0 or 1."},{"start":"92.94","dur":"6.53","text":"So the cost of this simple network for a single training example is (a^(L) - y)^2."},{"start":"100.25","dur":"4.4","text":"We’ll denote the cost of this one training example as C_0."},{"start":"106.03","dur":"5.49","text":"As a reminder, this last activation is determined by a weight, which I'm going to call w^(L)"},{"start":"111.98","dur":"2.24","text":"times the previous neuron’s activation,"},{"start":"114.53","dur":"2.41","text":"plus some bias, which I’ll call b^(L),"},{"start":"117.48","dur":"2.42","text":"then you pump that through some special nonlinear function"},{"start":"119.9","dur":"1.62","text":"like a sigmoid or a ReLU."},{"start":"121.85","dur":"5.13","text":"It's actually going to make things easier for us if we give a special name to this weighted sum, like z,"},{"start":"126.98","dur":"2.57","text":"with the same superscript as the relevant activations."},{"start":"130.39","dur":"1.09","text":"So there are a lot of terms."},{"start":"131.48","dur":"5.48","text":"And a way you might conceptualize this is that the weight, the previous activation, and the bias"},{"start":"136.96","dur":"4.44","text":"altogether are used to compute z, which in turn lets us compute a,"},{"start":"141.74","dur":"3.87","text":"which finally, along with the constant y, let us compute the cost."},{"start":"147.26","dur":"4.4","text":"And of course, a^(L-1) is influenced by its own weight and bias, and such."},{"start":"152.81","dur":"2.03","text":"But we are not gonna focus on that right now."},{"start":"155.68","dur":"2.36","text":"All of these are just numbers, right?"},{"start":"158.04","dur":"3.19","text":"And it can be nice to think of each one as having its own little number line."},{"start":"161.9","dur":"2.09","text":"Our first goal is to understand"},{"start":"163.99","dur":"4.95","text":"how sensitive the cost function is to small changes in our weight w^(L)."},{"start":"169.64","dur":"5.24","text":"Or phrased differently, what’s the derivative of C with respect to w^(L)."},{"start":"175.63","dur":"2.44","text":"When you see this “∂w” term,"},{"start":"178.07","dur":"4.68","text":"think of it as meaning “some tiny nudge to w”, like a change by 0.01."},{"start":"183.15","dur":"5.06","text":"And think of this “∂C” term as meaning “whatever the resulting nudge to the cost is”."},{"start":"188.71","dur":"1.71","text":"What we want is their ratio."},{"start":"191.21","dur":"5.31","text":"Conceptually, this tiny nudge to w^(L) causes some nudge to z^(L)"},{"start":"196.52","dur":"4.86","text":"which in turn causes some change to a^(L), which directly influences the cost."},{"start":"203.1","dur":"5.83","text":"So we break this up by first looking at the ratio of a tiny change to z^(L) to the tiny change in w^(L)."},{"start":"209.29","dur":"3.74","text":"That is, the derivative of z^(L) with respect to w^(L)."},{"start":"213.76","dur":"5.65","text":"Likewise, you then consider the ratio of a change to a^(L) to the tiny change in z^(L) that caused it,"},{"start":"219.85","dur":"5.03","text":"as well as the ratio between the final nudge to C and this intermediate nudge to a^(L)."},{"start":"225.67","dur":"2.18","text":"This right here is the chain rule,"},{"start":"227.85","dur":"7.1","text":"where multiplying together these three ratios gives us the sensitivity of C to small changes in w^(L)."},{"start":"237.19","dur":"2.85","text":"So on screen right now, there’s kinda lot of symbols,"},{"start":"240.04","dur":"2.96","text":"so take a moment to make sure it’s clear what they all are,"},{"start":"243.6","dur":"2.96","text":"because now we are gonna compute the relevant derivatives."},{"start":"247.4","dur":"5.83","text":"The derivative of C with respect to a^(L) works out to be 2(a^(L) - y)."},{"start":"253.96","dur":"2.92","text":"Notice, this means that its size is proportional to"},{"start":"256.88","dur":"4","text":"the difference between the network’s output, and the thing we want it to be."},{"start":"261.36","dur":"1.98","text":"So if that output was very different,"},{"start":"263.34","dur":"3.81","text":"even slight changes stand to have a big impact on the cost function."},{"start":"268.3","dur":"5.58","text":"The derivative of a^(L) with respect to z^(L) is just the derivative of our sigmoid function,"},{"start":"273.88","dur":"2.49","text":"or whatever nonlinearity you choose to use."},{"start":"277.31","dur":"3.06","text":"And the derivative of z^(L) with respect to w^(L),"},{"start":"281.47","dur":"3.06","text":"in this case comes out just to be a^(L-1)."},{"start":"286.07","dur":"3.5","text":"Now I don't know about you, but I think it’s easy to get stuck head-down in these formulas"},{"start":"289.57","dur":"4.12","text":"without taking a moment to sit back and remind yourself what they all actually mean."},{"start":"294.12","dur":"1.92","text":"In the case of this last derivative,"},{"start":"296.04","dur":"4.02","text":"the amount that a small nudge to this weight influences the last layer"},{"start":"300.06","dur":"2.79","text":"depends on how strong the previous neuron is."},{"start":"303.31","dur":"4.21","text":"Remember, this is where that “neurons that fire together wire together” idea comes in."},{"start":"309.21","dur":"6.73","text":"And all of this is the derivative with respect to w^(L) only of the cost for a specific training example."},{"start":"316.41","dur":"5.74","text":"Since the full cost function involves averaging together all those costs across many training examples,"},{"start":"322.15","dur":"5.46","text":"its derivative requires averaging this expression that we found over all training examples."},{"start":"328.43","dur":"3.5","text":"And of course that is just one component of the gradient vector,"},{"start":"331.93","dur":"1.96","text":"which itself is built up from"},{"start":"333.89","dur":"4.59","text":"the partial derivatives of the cost function with respect to all those weights and biases."},{"start":"340.71","dur":"2.84","text":"But even though it was just one of those partial derivatives we need,"},{"start":"343.55","dur":"1.84","text":"it's more than 50% of the work."},{"start":"346.42","dur":"3.52","text":"The sensitivity to the bias, for example, is almost identical."},{"start":"350.25","dur":"4.87","text":"We just need to change out this ∂z/∂w term for a ∂z/∂b,"},{"start":"358.76","dur":"3.83","text":"And if you look at the relevant formula, that derivative comes to be 1."},{"start":"366.21","dur":"3.67","text":"Also, and this is where the idea of propagating backwards comes in,"},{"start":"370.23","dur":"5.44","text":"you can see how sensitive this cost function is to the activation of the previous layer;"},{"start":"376.25","dur":"3.4","text":"namely, this initial derivative in the chain rule expansion,"},{"start":"379.65","dur":"3.45","text":"the sensitivity of z to the previous activation,"},{"start":"383.48","dur":"2.19","text":"comes out to be the weight w^(L)."},{"start":"386.58","dur":"4.92","text":"And again, even though we won’t be able to directly influence that activation,"},{"start":"391.5","dur":"1.58","text":"it’s helpful to keep track of,"},{"start":"393.08","dur":"5.12","text":"because now we can just keep iterating this chain rule idea backwards"},{"start":"398.2","dur":"4.55","text":"to see how sensitive the cost function is to previous weights and to previous biases."},{"start":"403.63","dur":"2.35","text":"And you might think this is an overly simple example,"},{"start":"405.98","dur":"1.9","text":"since all layers just have 1 neuron,"},{"start":"407.88","dur":"3.34","text":"and things are just gonna get exponentially more complicated in the real network."},{"start":"411.68","dur":"4.59","text":"But honestly, not that much changes when we give the layers multiple neurons."},{"start":"416.27","dur":"2.44","text":"Really it's just a few more indices to keep track of."},{"start":"419.34","dur":"3.54","text":"Rather than the activation of a given layer simply being a^(L),"},{"start":"422.88","dur":"4.33","text":"it's also going to have a subscript indicating which neuron of that layer it is."},{"start":"427.78","dur":"6.69","text":"Let’s go ahead and use the letter k to index the layer (L-1), and j to index the layer (L)."},{"start":"435.29","dur":"3.62","text":"For the the cost, again we look at what the desired output is."},{"start":"438.91","dur":"0.47","text":"But this time"},{"start":"439.38","dur":"5.88","text":"we add up the squares of the differences between these last layer activations and the desired output."},{"start":"446.06","dur":"5.01","text":"That is, you take a sum over (a_j^(L) - y_j)^2"},{"start":"453.11","dur":"1.41","text":"Since there are a lot more weights,"},{"start":"454.52","dur":"3.13","text":"each one has to have a couple more indices to keep track of where it is."},{"start":"458.01","dur":"6.98","text":"So let’s call the weight of the edge connecting this k-th neuron to the j-th neuron w_{jk}^(L)."},{"start":"465.66","dur":"2.6","text":"Those indices might feel a little backwards at first,"},{"start":"468.26","dur":"4.68","text":"but it lines up with how you’d index the weight matrix that I talked about in the Part 1 video."},{"start":"473.68","dur":"4.67","text":"Just as before, it’s still nice to give a name to the relevant weighted sum, like z,"},{"start":"478.35","dur":"5.96","text":"so that the activation of the last layer is just your special function, like the sigmoid, applied to z."},{"start":"485.04","dur":"1.19","text":"You can kinda see what I mean, right?"},{"start":"486.23","dur":"5.45","text":"These are all essentially the same equations we had before in the one-neuron-per-layer case;"},{"start":"491.68","dur":"2.19","text":"it just looks a little more complicated."},{"start":"495.37","dur":"2.85","text":"And indeed, the chain-rule derivative expression"},{"start":"498.22","dur":"3.76","text":"describing how sensitive the cost is to a specific weight"},{"start":"501.98","dur":"1.91","text":"looks essentially the same."},{"start":"503.89","dur":"2.99","text":"I’ll leave it to you to pause and think about each of these terms if you want."},{"start":"509.31","dur":"2.01","text":"What does change here, though,"},{"start":"511.32","dur":"5.51","text":"is the derivative of the cost with respect to one of the activations in the layer (L-1)."},{"start":"517.76","dur":"5.36","text":"In this case, the difference is the neuron influences the cost function through multiple paths."},{"start":"524.66","dur":"5.88","text":"That is, on the one hand, it influences a_0^(L), which plays a role in the cost function,"},{"start":"531.01","dur":"5.31","text":"but it also has an influence on a_1^(L), which also plays a role in the cost function."},{"start":"536.32","dur":"1.09","text":"And you have to add those up."},{"start":"540.17","dur":"2.81","text":"And that... well that is pretty much it."},{"start":"543.56","dur":"4.96","text":"Once you know how sensitive the cost function is to the activations in this second to last layer,"},{"start":"548.84","dur":"4.1","text":"you can just repeat the process for all the weights and biases feeding into that layer."},{"start":"553.85","dur":"1.51","text":"So pat yourself on the back!"},{"start":"555.36","dur":"1.59","text":"If this all of these makes sense,"},{"start":"556.95","dur":"3.49","text":"you have now looked deep into the heart of backpropagation,"},{"start":"560.44","dur":"2.39","text":"the workhorse behind how neural networks learn."},{"start":"563.59","dur":"5.71","text":"These chain rule expressions give you the derivatives that determine each component in the gradient"},{"start":"569.3","dur":"4.25","text":"that helps minimize the cost of the network by repeatedly stepping downhill."},{"start":"574.28","dur":"2.57","text":"Hhhhpf. If you sit back and think about all that,"},{"start":"576.85","dur":"3.24","text":"that’s a lot of layers of complexity to wrap your mind around."},{"start":"580.09","dur":"3","text":"So don't worry if it takes time for your mind to digest it all."}]